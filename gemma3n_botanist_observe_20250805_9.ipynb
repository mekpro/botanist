{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBia8GVKMvaS"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb-D9qhRMvaU"
      },
      "source": [
        "gemma3n_s1_botanist_20250721\n",
        "this is stage 1 of botanist.\n",
        "we first let the model see image and can specify the inflorescence , flower_arrangement, stem_pattern, flower_density, color ;\n",
        "as much as possible ;\n",
        "should need only one dict about inflorescence visual description because others are self explanatory.\n",
        "\n",
        "\n",
        "output in JSON;\n",
        "\n",
        "after the model can get image and return json of this.\n",
        "we will let the model think that from all visuals,\n",
        "the next stage will focus on using visual featues to confirm species name\n",
        "\n",
        "todo;\n",
        "read dataset mekpro/plantnet300k_grpo\n",
        "and construct json\n",
        "add dictionary inflorescence\n",
        "\n",
        "Instruction, as a botanist, describe this flower image in JSON format.\n",
        "\n",
        "answer\n",
        "{\n",
        "  \"color\" :\n",
        "  \"flower_arrangement\" :    \n",
        "  \"stem_pattern\" :        \n",
        "  \"flower_density\" :    \n",
        "  \"inflorescence\" :  \n",
        "  \"inflorescence_description\" :    \n",
        "  \"species_name\" :   \n",
        "}\n",
        "\n",
        "reward function :\n",
        "- inflorescence_species consistency :  if consistent + score , if not - score\n",
        "- color_species consistency :\n",
        "- flower_arrangement_species :   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3z2yeVsMvaV"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "pS-P_shPMvaX",
        "outputId": "26ff9395-398c-42fc-ee35-abe8ff790fba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\\n    !pip install unsloth\\nelse:\\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\\n    !pip install --no-deps unsloth\\n\\n# Install latest transformers for Gemma 3N\\n!pip install --no-deps --upgrade timm # Only for Gemma 3N\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "# Install latest transformers for Gemma 3N\n",
        "!pip install --no-deps --upgrade timm # Only for Gemma 3N\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i7qKiuK36wV",
        "outputId": "deef0845-7449-4ef1-ef54-cc52f1c3746c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "output_dir = f\"/content/drive/MyDrive/colab_output/gemma-3n-botanist9\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "d78f54192a824a83bb3d88881e6b51cc",
            "bec8ca1a2de44673ab847cd97d033d22",
            "a25fbd4cba664afbbe3daf1c8295d265",
            "f2b9e3c83c924417ade3fd3f3ef70e07",
            "701762ad33cc4e60a945bf7167b3a218",
            "6a37cc4036b2463c928fa6765dc2a51b",
            "67f78fd9eb484237a405a2a5655f182e",
            "b6be96af04d9475fa9188f530c422b66",
            "fc0e046dfb8f4b8fa378d922b9eef642",
            "60856ac986cc4efea15f093eb322cd00",
            "1e24a0fe29d749e0b89b15b286bfafb2"
          ]
        },
        "id": "-Xbb0cuLzwgf",
        "outputId": "7c258689-29f4-4676-d579-1113ea0fe9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Gemma3N does not support SDPA - switching to eager!\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d78f54192a824a83bb3d88881e6b51cc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "torch._dynamo.config.suppress_errors = True  # Optional: suppress if errors occur\n",
        "\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3n-E4B-it-litert-preview\",\n",
        "    #model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    dtype = torch.bfloat16, # None for auto detection\n",
        "    max_seq_length = 512, # Choose any for long context!\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    #use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixr4dyTHVIcI"
      },
      "source": [
        "# Gemma 3N can process Text, Vision and Audio!\n",
        "\n",
        "Let's first experience how Gemma 3N can handle multimodal inputs. We use Gemma 3N's recommended settings of `temperature = 1.0, top_p = 0.95, top_k = 64`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2ZL6knoYm84D"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "# Helper function for inference\n",
        "def do_gemma_3n_inference(messages, max_new_tokens = 128):\n",
        "    _ = model.generate(\n",
        "        **tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt = True, # Must add for generation\n",
        "            tokenize = True,\n",
        "            return_dict = True,\n",
        "            return_tensors = \"pt\",\n",
        "        ).to(\"cuda\"),\n",
        "        max_new_tokens = max_new_tokens,\n",
        "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XPa8P3v6-DMT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2-ddk0CWeTA"
      },
      "source": [
        "# Gemma 3N can see images!\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/plantnet/PlantNet-300K/refs/heads/main/images/1.jpg\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9jGeSb9bWe0k"
      },
      "outputs": [],
      "source": [
        "link = \"https://raw.githubusercontent.com/plantnet/PlantNet-300K/refs/heads/main/images/1.jpg\" # Cirsium arvense\n",
        "\n",
        "messages = [{\n",
        "    \"role\" : \"user\",\n",
        "    \"content\": [\n",
        "        { \"type\": \"image\", \"image\" : link },\n",
        "        { \"type\": \"text\",  \"text\" : \"As a botanist, specify inflorescence and speices name of this flower.\" }\n",
        "    ]\n",
        "}]\n",
        "\n",
        "#do_gemma_3n_inference(messages, max_new_tokens = 50)\n",
        "\n",
        "#print(\"broken after here\")\n",
        "#from unsloth.chat_templates import get_chat_template\n",
        "#get_chat_template(tokenizer, chat_template = \"gemma-3\")\n",
        "#do_gemma_3n_inference(messages, max_new_tokens = 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw5XPyYFajyM"
      },
      "source": [
        "# Let's finetune Gemma 3N!\n",
        "\n",
        "You can finetune the vision and text parts for now through selection - the audio part can also be finetuned - we're working to make it selectable as well!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "c15a14d3-ab82-413d-82f4-2b7bc70693cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 256,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 256,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0.2,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Gemma-3` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. Gemma-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<bos><start_of_turn>user\n",
        "Hello!<end_of_turn>\n",
        "<start_of_turn>model\n",
        "Hey there!<end_of_turn>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQkXuGYxbJ-e"
      },
      "source": [
        "We get the first 3000 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkq4RvEq7FQr",
        "outputId": "91e9b72c-f588-45ed-9b7b-b43abb5bf7f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 19754\n",
            "Validation size: 1487\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "full_train_dataset = load_dataset(\"mekpro/plantnet300k_observe\", split=\"train[:30000]\")\n",
        "split_datasets = full_train_dataset.train_test_split(test_size=0.07, seed=42)\n",
        "train_dataset = split_datasets['train']\n",
        "val_dataset = split_datasets['test']  # 'test' here is actually our validation set\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We now use `standardize_data_formats` to try converting datasets to the correct format for finetuning purposes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQpSBNziWNIY",
        "outputId": "04834043-e728-4694-a8fd-06f68aad52d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'user',\n",
              "   'content': [{'type': 'image',\n",
              "     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=384x384>},\n",
              "    {'type': 'text',\n",
              "     'text': 'As a botanist, observe the image of the flower and describe its visual features and species_name in JSON format. {color, inflorescencetype, inflorescence_description, flower_arrangement, flower_density, species, family, genus}'}]},\n",
              "  {'role': 'assistant',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': '{\"color\": \"Bicolor; white petals with a prominent, feathered pink \\'eye\\' or throat at the center.\", \"inflorescencetype\": \"Pseudo-umbel\", \"inflorescence_description\": \"A rounded flower head on an erect stalk, with the bicolored pattern creating a striking effect.\", \"flower_arrangement\": \"Individual flowers are clustered tightly, with the central color of each creating a stippled look.\", \"flower_density\": \"Dense\", \"species\": \"Pelargonium zonale\", \"family\": \"Geraniaceae\", \"genus\": \"Pelargonium\"}'}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "instruction = \"As a botanist, observe the image of the flower and describe its visual features and species_name in JSON format. {color, inflorescencetype, inflorescence_description, flower_arrangement, flower_density, species, family, genus}\"\n",
        "def convert_to_conversation(sample):\n",
        "     result = {\n",
        "          \"color\" : sample[\"color\"],\n",
        "          \"inflorescencetype\"  : sample[\"inflorescencetype\"],\n",
        "          \"inflorescence_description\" : sample[\"inflorescence_description\"],\n",
        "          \"flower_arrangement\" : sample[\"flower_arrangement\"],\n",
        "          \"flower_density\" : sample[\"flower_density\"],\n",
        "#          \"unique_visual_description\" : sample [\"unique_visual_description\"],\n",
        "#          \"morphological_traits_observable_in_photograph\": sample[\"morphological_traits_observable_in_photograph\"],\n",
        "          \"species\" : sample[\"species\"],\n",
        "          \"family\" : sample[\"family\"],\n",
        "          \"genus\" : sample[\"genus\"],\n",
        " #         \"visual_contrast_with_similar_species\" : sample[\"visual_contrast_with_similar_species\"]\n",
        "     }\n",
        "     conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
        "                {\"type\": \"text\", \"text\": instruction},\n",
        "            ],\n",
        "        },\n",
        "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": json.dumps(result) }]},\n",
        "     ]\n",
        "     return {\"messages\": conversation}\n",
        "train_dataset_c = [convert_to_conversation(sample) for sample in train_dataset]\n",
        "val_dataset_c = [convert_to_conversation(sample) for sample in val_dataset]\n",
        "train_dataset_c[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "\n",
        "plant_augmentations = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomResizedCrop(size=(384, 384), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "# Custom collator with augmentation\n",
        "class AugmentedUnslothVisionDataCollator(UnslothVisionDataCollator):\n",
        "    def __init__(self, processor, image_processor, is_training=True):\n",
        "        super().__init__(processor, image_processor)\n",
        "        self.is_training = is_training\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # Apply augmentation only if training flag is set\n",
        "        if self.is_training:\n",
        "            for feature in features:\n",
        "                if 'image' in feature and feature['image'] is not None:\n",
        "                    img = feature['image']\n",
        "                    if not isinstance(img, Image.Image):\n",
        "                        img = Image.fromarray(img)\n",
        "                    feature['image'] = plant_augmentations(img)\n",
        "\n",
        "        # Call parent collator\n",
        "        return super().__call__(features)\n",
        "\n",
        "\n",
        "train_collator = AugmentedUnslothVisionDataCollator(model, tokenizer, is_training=True)\n",
        "eval_collator = AugmentedUnslothVisionDataCollator(model, tokenizer, is_training=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF_QF5o62ebX",
        "outputId": "42fd0637-a240-47fe-8a00-0ddf6df103e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Model does not have a default image size - using 512\n",
            "Unsloth: Model does not have a default image size - using 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reoBXmAn7HlN",
        "outputId": "b894a60b-1b1b-432a-84bb-b6cb0805aab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "<image_soft_token>As a botanist, observe the image of the flower and describe its visual features and species_name in JSON format. {color, inflorescencetype, inflorescence_description, flower_arrangement, flower_density, species, family, genus}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\"color\": \"Vibrant sky-blue petals fading to a white center, with prominent dark purple anthers.\", \"inflorescencetype\": \"Cyme\", \"inflorescence_description\": \"Loose, flat-topped terminal cymes with numerous small flowers opening sequentially.\", \"flower_arrangement\": \"Small, star-like flowers arranged in branching clusters at the apex of the stems.\", \"flower_density\": \"Moderately dense\", \"species\": \"Sedum caeruleum\", \"family\": \"Crassulaceae\", \"genus\": \"Sedum\"}<end_of_turn>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from unsloth.chat_templates import standardize_data_formats\n",
        "train_dataset_c_s = standardize_data_formats(train_dataset_c)\n",
        "val_dataset_c_s = standardize_data_formats(val_dataset_c)\n",
        "print(tokenizer.apply_chat_template(train_dataset_c_s[10][\"messages\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCPjcO4pWGTL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xs0LXio7rfd"
      },
      "source": [
        "We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "Let's see how the chat template did! Notice there is no `<bos>` token as the processor tokenizer will be adding one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be8hUh7yYHX7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime;\n",
        "run_name = \"gemma-3n-botanist\"+(datetime.now().strftime('_%y%m%d-%H%M'))\n",
        "\n",
        "FastModel.for_training(model) # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset_c,\n",
        "    eval_dataset=val_dataset_c,\n",
        "    processing_class=tokenizer.tokenizer,\n",
        "    data_collator=train_collator,\n",
        "    eval_data_collator=eval_collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        "    args = SFTConfig(\n",
        "        per_device_eval_batch_size = 24,\n",
        "        per_device_train_batch_size = 24,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        #gradient_checkpointing = False,\n",
        "        #gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
        "        max_grad_norm = 0.5,              # max gradient norm based on QLoRA paper\n",
        "        warmup_steps = 20,\n",
        "        max_steps = -1,\n",
        "        num_train_epochs = 4,          # Set this instead of max_steps for full training runs\n",
        "        learning_rate = 3e-4,\n",
        "        lr_scheduler_type = \"cosine_with_min_lr\",\n",
        "        lr_scheduler_kwargs= { \"min_lr\": 1e-4 },\n",
        "        logging_steps = 1,\n",
        "        eval_steps = 200,\n",
        "        save_steps = 200,\n",
        "        eval_strategy = \"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        optim = \"adamw_torch_fused\",\n",
        "        weight_decay = 0.05,\n",
        "        seed = 3407,\n",
        "        output_dir = output_dir ,\n",
        "        # âš¡ Key settings for GPU utilization\n",
        "        dataloader_num_workers=8,\n",
        "        dataloader_pin_memory=True,  # Faster GPU transfer\n",
        "        dataloader_prefetch_factor=2,  # Prefetch batches\n",
        "        bf16=True,\n",
        "        report_to = \"wandb\",\n",
        "        run_name = run_name,  # Name of the run in wandb\n",
        "\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='eval_loss',\n",
        "        greater_is_better=False,\n",
        "\n",
        "\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        #max_seq_length = 1024,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SLBnHRxLvlx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "022f44f8-e2ff-46ff-e3a5-b3e44bd48fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 19,754 | Num Epochs = 4 | Total steps = 3,296\n",
            "O^O/ \\_/ \\    Batch size per device = 24 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (24 x 1 x 1) = 24\n",
            " \"-____-\"     Trainable parameters = 614,727,680 of 8,464,705,872 (7.26% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmekpro\u001b[0m (\u001b[33mmekproai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250805_144953-rurtalcc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mekproai/huggingface/runs/rurtalcc' target=\"_blank\">gemma-3n-botanist_250805-1449</a></strong> to <a href='https://wandb.ai/mekproai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mekproai/huggingface' target=\"_blank\">https://wandb.ai/mekproai/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mekproai/huggingface/runs/rurtalcc' target=\"_blank\">https://wandb.ai/mekproai/huggingface/runs/rurtalcc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1694' max='3296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1694/3296 2:12:19 < 2:05:16, 0.21 it/s, Epoch 2.05/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.379600</td>\n",
              "      <td>0.589601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.252800</td>\n",
              "      <td>0.491036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.106300</td>\n",
              "      <td>0.290566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.124800</td>\n",
              "      <td>0.199648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.087800</td>\n",
              "      <td>0.194192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.084900</td>\n",
              "      <td>0.186292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.053600</td>\n",
              "      <td>0.148489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>0.131423</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.config.cache_size_limit = 256  # Default is 64, increase as needed\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uaCbKEAcWbSt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrsRi_hLaoHR"
      },
      "outputs": [],
      "source": [
        "# write a botanist test , give model image_url and prompt -> predict species , and , inflorencetype , validate species and inflorencetype. count score\n",
        "# 1. prepare image_url from dataset upload to s3 public, map with list of species and inflorencetype , return list of (image_url, species, inflorencetype)\n",
        "# llm_inference_tester,\n",
        "# : read the csv (image_url, species, inflorescencetype) , https://mekpro.ap-south-1.linodeobjects.com/botanist/random_sample_100.csv\n",
        "# test by inference each one, with image text, get json result, parse json check species and inflorescencetype , count correct species , count correct inflorescencetype\n",
        "# image domain prefix is: https://mekpro.ap-south-1.linodeobjects.com/botanist/\n",
        "# prompt is  \"As a botanist, observe the image of the flower and describe its visual features and species_name in JSON format. {color, inflorescencetype, inflorescence_description, flower_arrangement, flower_density, unique_visual_description, morphological_traits_observable_in_photograph, species, family, genus,  visual_contrast_with_similar_species}\"\n",
        "\n",
        "\n",
        "# https://mekpro.ap-south-1.linodeobjects.com/botanist%2Ftrain%2F1355868%2F37975562c06628d9d3e38e03df6051c4b1c4f692.jpg\n",
        "\n",
        "\n",
        "link = \"https://raw.githubusercontent.com/plantnet/PlantNet-300K/refs/heads/main/images/1.jpg\" # Cirsium arvense\n",
        "#link = \"https://mekpro.ap-south-1.linodeobjects.com/pelargonium_peltatum_4.jpg\"\n",
        "instruction = \"As a botanist, observe the image of the flower and describe its visual features and species_name in JSON format. {color, inflorescencetype, inflorescence_description, flower_arrangement, flower_density, species, family, genus}\"\n",
        "FastModel.for_inference(model) # Enable for training!\n",
        "\n",
        "\n",
        "messages = [{\n",
        "    \"role\" : \"user\",\n",
        "    \"content\": [\n",
        "        { \"type\": \"image\", \"image\" : link },\n",
        "        { \"type\": \"text\",  \"text\" : instruction }\n",
        "    ]\n",
        "}]\n",
        "\n",
        "# You might have to wait 1 minute for Unsloth's auto compiler\n",
        "do_gemma_3n_inference(messages, max_new_tokens = 512)\n",
        "\n",
        "messages = [{\n",
        "    \"role\" : \"user\",\n",
        "    \"content\": [\n",
        "        { \"type\": \"text\",  \"text\" : instruction }\n",
        "    ]\n",
        "}]\n",
        "\n",
        "#do_gemma_3n_inference(messages, max_new_tokens = 512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d38Dnd-xlCWa"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi,login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "#!huggingface-cli repo-files \"mekpro/gemma-3n-e4b-gemma-3n-botanist-observe\" delete \"*\"\n",
        "#!huggingface-cli repo-files \"mekpro/gemma-3n-e4b-gemma-3n-botanist-observe-merged\" delete \"*\"\n",
        "#!huggingface-cli repo-files \"mekpro/gemma-3n-e4b-gemma-3n-botanist-observe-gguf\" delete \"*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "# model.save_pretrained(\"gemma-3n\")  # Local saving\n",
        "# tokenizer.save_pretrained(\"gemma-3n\")\n",
        "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "from huggingface_hub import HfApi,login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "import transformers\n",
        "model_save_path = f\"gemma-3n-botanist8\"\n",
        "\n",
        "model.save_pretrained(output_dir)  # Local saving\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "model.save_pretrained_merged(output_dir+\"-merged\", tokenizer)\n",
        "model.push_to_hub(\"mekpro/\"+model_save_path, token = HF_TOKEN) # Online saving\n",
        "tokenizer.push_to_hub(\"mekpro/\"+model_save_path, token = HF_TOKEN) # Online saving\n",
        "\n",
        "\n",
        "model.push_to_hub_merged(\n",
        "    repo_id=\"mekpro/\"+model_save_path+\"-merged\",\n",
        "    tokenizer=tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        "    token = HF_TOKEN\n",
        ")\n",
        "\n",
        "#model.push_to_hub_gguf(model_save_path+\"-gguf\", repo_id=\"mekpro/gemma-3n-e4b-botanist-gguf-grpo\", quantization_type=\"q8_0\", token=HF_TOKEN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG-QBzdusU3j"
      },
      "source": [
        "For clear file (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXxQcDq1ov9S"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PSnvi7OouO2"
      },
      "source": [
        "staty connected\n",
        "\n",
        "\n",
        "\n",
        "function ClickConnect() {\n",
        "    console.log(\"Clicked on connect button\");\n",
        "    document.querySelector(\"colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "}\n",
        "setInterval(ClickConnect, 60000);\n",
        "\n",
        "monitoring:\n",
        "\n",
        "\n",
        "nvidia-smi --query-gpu=power.draw,memory.used,memory.total,utilization.memory,utilization.gpu,clocks.current.graphics,clocks.current.sm,clocks.current.memory,clocks.current.video -l 1 --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBd-mW-ldgV7"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d78f54192a824a83bb3d88881e6b51cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bec8ca1a2de44673ab847cd97d033d22",
              "IPY_MODEL_a25fbd4cba664afbbe3daf1c8295d265",
              "IPY_MODEL_f2b9e3c83c924417ade3fd3f3ef70e07"
            ],
            "layout": "IPY_MODEL_701762ad33cc4e60a945bf7167b3a218"
          }
        },
        "bec8ca1a2de44673ab847cd97d033d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a37cc4036b2463c928fa6765dc2a51b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_67f78fd9eb484237a405a2a5655f182e",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "a25fbd4cba664afbbe3daf1c8295d265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6be96af04d9475fa9188f530c422b66",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc0e046dfb8f4b8fa378d922b9eef642",
            "value": 4
          }
        },
        "f2b9e3c83c924417ade3fd3f3ef70e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60856ac986cc4efea15f093eb322cd00",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1e24a0fe29d749e0b89b15b286bfafb2",
            "value": "â€‡4/4â€‡[01:37&lt;00:00,â€‡23.75s/it]"
          }
        },
        "701762ad33cc4e60a945bf7167b3a218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a37cc4036b2463c928fa6765dc2a51b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67f78fd9eb484237a405a2a5655f182e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6be96af04d9475fa9188f530c422b66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc0e046dfb8f4b8fa378d922b9eef642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60856ac986cc4efea15f093eb322cd00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e24a0fe29d749e0b89b15b286bfafb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}